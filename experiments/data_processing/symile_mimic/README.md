# Symile-MIMIC
<img src="/img/symile_mimic.png" alt="Symile-MIMIC" width="400"/>

In this section, we describe how to generate Symile-MIMIC, a clinical dataset of chest X-rays (CXRs), electrocardiograms (ECGs), and blood labs from the MIMIC-IV and MIMIC-CXR datasets. Download the Symile-MIMIC dataset [here](https://doi.org/10.13026/3vvj-s428) and see our [paper](https://arxiv.org/abs/2411.01053) for details on the dataset.

## Download MIMIC data

Symile-MIMIC is generated by sampling CXRs, ECGs, and labs from the following MIMIC datasets, which you will need to download:
- **MIMIC-IV:** Download admissions and labs data [here](https://physionet.org/content/mimiciv/2.2/).
- **MIMIC-IV-ECG:** Download ECG data [here](https://physionet.org/content/mimic-iv-ecg/1.0/).
- **MIMIC-CXR-JPG:** Download CXR data in JPG format [here](https://physionet.org/content/mimic-cxr-jpg/2.0.0/).

## Activate environment

To start, make sure you're in the correct directory:
```
> cd experiments/data_processing/symile_mimic
```

#### conda

```
> conda env create -f env/environment.yml
> conda activate symile-mimic
(symile-mimic) >
```

#### pip

```
> python -m venv venv
> source venv/bin/activate
(venv) > pip install -r env/requirements.txt
```

## Process MIMIC data

The following command processes the MIMIC data to generate the Symile-MIMIC dataset, which is saved to `save_dir` as `symile_mimic_data.csv`:

```
(symile-env) > python process_mimic_data.py [FLAGS]
```

This command takes the following flags:

| Flag                   | Description                                                                                       | Type   |
|------------------------|---------------------------------------------------------------------------------------------------|--------|
| `--mimiciv_hosp_dir`   | Path to MIMIC-IV hospital module directory, which must include the files `patients.csv.gz`, `admissions.csv.gz`, and `labevents.csv.gz`. | str    |
| `--cxr_data_dir`       | Directory with MIMIC CXR data, which must include the files `mimic-cxr-2.0.0-metadata.csv.gz` and `mimic-cxr-2.0.0-chexpert.csv.gz`.   | str    |
| `--ecg_data_dir`       | Directory with MIMIC ECG data, which must include the file `record_list.csv`.                                                         | str    |
| `--save_dir`           | Directory where the DataFrame with processed data will be saved.                                                                      | str    |

The script typically completes in 7 hours when executed with 16 CPUs and 100GB of memory.

Each data sample includes an ECG reading and blood labs taken within 24 hours of the patient's admission to the hospital, and a CXR taken in the 24-72 hour period post-admission. For each admission, we choose the earliest CXR, ECG, and labs. Only CXRs with a posteroanterior (PA) or anteroposterior (AP) view are considered, and any ECGs with NaN values or with a signal of all zeros are removed from consideration. Labs are filtered to include only the 50 most common blood laboratory measurements (see `constants.py`). Each admission has a CXR, an ECG, and at least one of the 50 lab values.

## Create dataset splits

The following command creates dataset splits from the Symile-MIMIC dataset CSV saved by the previous command:

```
(symile-env) > python create_dataset_splits.py [FLAGS]
```

This command takes the following flags:

| Flag               | Description                                              | Type         | Default |
|--------------------|----------------------------------------------------------|--------------|---------|
| `--dataset_path`   | Path to CSV file with the full dataset.                  | str       |     |
| `--save_dir`       | Directory where the data will be saved.                  | str       |     |
| `--train_n`        | Number of samples in the training set.                   | `int`        |     |
| `--val_n`          | Number of samples in the validation set.                 | `int`        |     |
| `--candidate_n`    | Number of candidates for each test sample (`candidate_n - 1` negative candidates for each). | `int`        |     |
| `--seed`           | Seed value for random number generation.                 | int        | 0       |
| `--use_seed`       | Whether to use a seed for reproducibility.               | bool| `True`    |

The dataset is split into training, validation, and test sets ensuring there is no patient overlap between the splits. Lab values are converted into percentiles using a NaN-aware empirical cumulative distribution function.

To build the evaluation sets for Symile-MIMIC (`val_retrieval.csv` and `test.csv`), we treat each data sample as a query for the CXR retrieval task. For each query, we sample 9 negative candidates from the remaining data in the respective split, ensuring that each query has a total of 10 candidates: 1 positive (the query itself) and 9 negatives.

## Process and save dataset tensors

To accelerate training, the following command loads and preprocesses the Symile-MIMIC dataset splits, saving the
resulting tensors to split-specific directories in `data_dir`:

```
(symile-env) > python process_and_save_tensors.py [FLAGS]
```

This command takes the following flags:

| Flag               | Description                                              | Type         | Default          |
|--------------------|----------------------------------------------------------|--------------|------------------|
| `--data_dir`       | Directory with dataset csvs.                             | str       |              |
| `--ecg_data_dir`   | Directory that contains the MIMIC `files` directory with ECG data. | str       |              |
| `--cxr_data_dir`   | Directory that contains the MIMIC `files` directory with CXR data. | str       |              |
| `--labs_means`     | JSON filename for labs means.                            | str       | `labs_means.json`|
| `--train_csv`      | Filename for train csv.                                  | str       | `train.csv`      |
| `--val_csv`        | Filename for val csv.                                    | str       | `val.csv`        |
| `--val_retrieval_csv`    | Filename for val accuracy csv.                           | str       | `val_retrieval.csv`    |
| `--test_csv`       | Filename for test csv.                                   | str       | `test.csv`       |
| `--cxr_scale`      | Scale for preprocessing CXRs.                            | int        | 320              |
| `--cxr_crop`       | Crop for preprocessing CXRs.                             | int        | 320              |

The script typically completes in 1 hour when executed with 16 CPUs and 150GB of memory.

Following [CheXpert](https://github.com/stanfordmlgroup/chexpert-model), each CXR is scaled such that the smaller edge is set to `cxr_scale = 320`, followed by a square crop (random for training or center for validation and testing). Images are then normalized using the ImageNet mean and standard deviation. The ECG signal is normalized to lie within the range [-1, 1].

Eventually, for the labs model, we will use a 100-dimensional vector as input: the first 50 coordinates are lab values standardized to percentiles based on the training set's empirical CDF, and the remaining 50 coordinates are binary indicators that denote whether each lab value is missing. When a lab value is unobserved, the mean percentile for that lab is substituted. This script computes the percentiles and the missingness indicators for the lab values, and then saves them as separate tensors.
